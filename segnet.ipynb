{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input\n",
      "/kaggle/input/isic2018\n",
      "/kaggle/input/isic2018/isic2018_task1-2_validation_input\n",
      "/kaggle/input/isic2018/isic2018_task1-2_validation_input/ISIC2018_Task1-2_Validation_Input\n",
      "/kaggle/input/isic2018/isic2018_task1-2_training_input\n",
      "/kaggle/input/isic2018/isic2018_task1-2_training_input/ISIC2018_Task1-2_Training_Input\n",
      "/kaggle/input/isic2018/isic2018_task1_training_groundtruth\n",
      "/kaggle/input/isic2018/isic2018_task1_training_groundtruth/ISIC2018_Task1_Training_GroundTruth\n",
      "/kaggle/input/isic2018/ISIC2018_Task1_Training_GroundTruth\n",
      "/kaggle/input/isic2018/ISIC2018_Task1_Training_GroundTruth/ISIC2018_Task1_Training_GroundTruth\n",
      "/kaggle/input/isic2018/isic2018_task1-2_test_input\n",
      "/kaggle/input/isic2018/isic2018_task1-2_test_input/ISIC2018_Task1-2_Test_Input\n",
      "/kaggle/input/isic2018/ISIC2018_Task1-2_Training_Input\n",
      "/kaggle/input/isic2018/ISIC2018_Task1-2_Training_Input/ISIC2018_Task1-2_Training_Input\n",
      "/kaggle/input/isic2018/ISIC2018_Task1-2_Test_Input\n",
      "/kaggle/input/isic2018/ISIC2018_Task1-2_Test_Input/ISIC2018_Task1-2_Test_Input\n",
      "/kaggle/input/isic2018/ISIC2018_Task1-2_Validation_Input\n",
      "/kaggle/input/isic2018/ISIC2018_Task1-2_Validation_Input/ISIC2018_Task1-2_Validation_Input\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    print(dirname)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ISIC 2018\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "Reading ISIC 2018 finished\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import scipy.io as sio\n",
    "import scipy.misc as sc\n",
    "from imageio import imread\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "# Parameters\n",
    "height = 256\n",
    "width  = 256\n",
    "channels = 3\n",
    "Dataset_add = '/kaggle/input/isic2018/isic2018_task1-2_training_input'\n",
    "Tr_add = 'ISIC2018_Task1-2_Training_Input'\n",
    "\n",
    "Tr_list = glob.glob(Dataset_add+'/'+Tr_add+'/*.jpg')\n",
    "\n",
    "# It contains 2594 training samples\n",
    "Data_train_2018    = np.zeros([2594, height, width, channels])\n",
    "Label_train_2018   = np.zeros([2594, height, width])\n",
    "\n",
    "print('Reading ISIC 2018')\n",
    "for idx in range(len(Tr_list)):\n",
    "    if (idx+1)%50 == 0:\n",
    "        print(idx+1)\n",
    "    img = cv2.imread(Tr_list[idx])\n",
    "    img = np.double(cv2.resize(img,(width,height),interpolation=cv2.INTER_LINEAR))\n",
    "    Data_train_2018[idx, :,:,:] = img\n",
    "\n",
    "    b = Tr_list[idx]    \n",
    "    a = b[0:len(Dataset_add)]\n",
    "    b = b[len(b)-16: len(b)-4] \n",
    "    add = ('/kaggle/input/isic2018/ISIC2018_Task1_Training_GroundTruth/ISIC2018_Task1_Training_GroundTruth/' + b +'_segmentation.png')    \n",
    "    img2 = cv2.imread(add,0)\n",
    "    img2 = cv2.resize(img2,(width,height),interpolation=cv2.INTER_LINEAR)\n",
    "    Label_train_2018[idx, :,:] = img2    \n",
    "         \n",
    "print('Reading ISIC 2018 finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_img      = Data_train_2018[0:1815,:,:,:]\n",
    "Validation_img = Data_train_2018[1815:1815+259,:,:,:]\n",
    "Test_img       = Data_train_2018[1815+259:2394,:,:,:]\n",
    "\n",
    "Train_mask      = Label_train_2018[0:1815,:,:]\n",
    "Validation_mask = Label_train_2018[1815:1815+259,:,:]\n",
    "Test_mask       = Label_train_2018[1815+259:2394,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model as plot\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *  \n",
    "\n",
    "def SegNet(nClasses=1, input_height=256, input_width=256):\n",
    "    img_input = Input(shape=(input_height, input_width, 3))\n",
    "    kernel_size = 3\n",
    "    # encoder\n",
    "    x = Conv2D(64, (kernel_size, kernel_size), padding='same',\n",
    "               kernel_initializer='he_normal')(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 128x128\n",
    "    x = Conv2D(128, (kernel_size, kernel_size), padding='same',\n",
    "               kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 64x64\n",
    "    x = Conv2D(256, (kernel_size, kernel_size), padding='same',\n",
    "               kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 32x32\n",
    "    x = Conv2D(512, (kernel_size, kernel_size), padding='same',\n",
    "               kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 16x16\n",
    "    x = Conv2D(512, (kernel_size, kernel_size), padding='same',\n",
    "               kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # 8x8\n",
    "\n",
    "    # decoder\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(512, (kernel_size, kernel_size), padding='same',\n",
    "               kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(512, (kernel_size, kernel_size), padding='same',\n",
    "               kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(256, (kernel_size, kernel_size), padding='same',\n",
    "               kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(128, (kernel_size, kernel_size), padding='same',\n",
    "               kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(64, (kernel_size, kernel_size), padding='same',\n",
    "               kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(nClasses, (1, 1), padding='valid',\n",
    "               kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Activation('sigmoid')(x)\n",
    "    model = Model(img_input, x, name='SegNet')\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIC18 Dataset loaded\n",
      "dataset Normalized\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard,ReduceLROnPlateau\n",
    "from keras import callbacks\n",
    "import pickle\n",
    "\n",
    "\n",
    "# ===== normalize over the dataset \n",
    "def dataset_normalized(imgs):\n",
    "    imgs_normalized = np.empty(imgs.shape)\n",
    "    imgs_std = np.std(imgs)\n",
    "    imgs_mean = np.mean(imgs)\n",
    "    imgs_normalized = (imgs-imgs_mean)/imgs_std\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_normalized[i] = ((imgs_normalized[i] - np.min(imgs_normalized[i])) / (np.max(imgs_normalized[i])-np.min(imgs_normalized[i])))*255\n",
    "    return imgs_normalized\n",
    "\n",
    "tr_data = Train_img\n",
    "te_data = Test_img\n",
    "val_data = Validation_img\n",
    "\n",
    "tr_mask = Train_mask\n",
    "te_mask = Test_mask\n",
    "val_mask = Validation_mask\n",
    "\n",
    "tr_mask    = np.expand_dims(tr_mask, axis=3)\n",
    "te_mask    = np.expand_dims(te_mask, axis=3)\n",
    "val_mask   = np.expand_dims(val_mask, axis=3)\n",
    "\n",
    "print('ISIC18 Dataset loaded')\n",
    "tr_data   = dataset_normalized(tr_data)\n",
    "te_data   = dataset_normalized(te_data)\n",
    "val_data  = dataset_normalized(val_data)\n",
    "\n",
    "tr_mask   = tr_mask /255.\n",
    "te_mask   = te_mask /255.\n",
    "val_mask  = val_mask /255.\n",
    "\n",
    "print('dataset Normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SegNet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 64, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 128, 128, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 128, 128, 128)     295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 256, 256, 64)      73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 256, 256, 1)       65        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256, 256, 1)       0         \n",
      "=================================================================\n",
      "Total params: 10,190,977\n",
      "Trainable params: 10,185,089\n",
      "Non-trainable params: 5,888\n",
      "_________________________________________________________________\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks/callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1815 samples, validate on 259 samples\n",
      "Epoch 1/30\n",
      "1815/1815 [==============================] - 53s 29ms/step - loss: 0.4029 - accuracy: 0.8612 - val_loss: 1.0627 - val_accuracy: 0.6240\n",
      "Epoch 2/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.2922 - accuracy: 0.9072 - val_loss: 0.2924 - val_accuracy: 0.9082\n",
      "Epoch 3/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.2489 - accuracy: 0.9202 - val_loss: 0.2787 - val_accuracy: 0.9192\n",
      "Epoch 4/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.2254 - accuracy: 0.9246 - val_loss: 0.2586 - val_accuracy: 0.9127\n",
      "Epoch 5/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.2071 - accuracy: 0.9301 - val_loss: 0.2209 - val_accuracy: 0.9211\n",
      "Epoch 6/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1871 - accuracy: 0.9351 - val_loss: 0.2216 - val_accuracy: 0.9256\n",
      "Epoch 7/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1824 - accuracy: 0.9353 - val_loss: 0.2022 - val_accuracy: 0.9308\n",
      "Epoch 8/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1765 - accuracy: 0.9377 - val_loss: 0.1961 - val_accuracy: 0.9307\n",
      "Epoch 9/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1629 - accuracy: 0.9420 - val_loss: 0.2094 - val_accuracy: 0.9280\n",
      "Epoch 10/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1617 - accuracy: 0.9413 - val_loss: 0.2113 - val_accuracy: 0.9297\n",
      "Epoch 11/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1492 - accuracy: 0.9448 - val_loss: 0.1800 - val_accuracy: 0.9351\n",
      "Epoch 12/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1481 - accuracy: 0.9456 - val_loss: 0.1869 - val_accuracy: 0.9354\n",
      "Epoch 13/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1391 - accuracy: 0.9481 - val_loss: 0.1892 - val_accuracy: 0.9311\n",
      "Epoch 14/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1435 - accuracy: 0.9471 - val_loss: 0.1860 - val_accuracy: 0.9342\n",
      "Epoch 15/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1288 - accuracy: 0.9522 - val_loss: 0.1966 - val_accuracy: 0.9329\n",
      "Epoch 16/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1278 - accuracy: 0.9519 - val_loss: 0.2358 - val_accuracy: 0.9169\n",
      "Epoch 17/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1254 - accuracy: 0.9536 - val_loss: 0.1933 - val_accuracy: 0.9327\n",
      "Epoch 18/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1208 - accuracy: 0.9544 - val_loss: 0.2008 - val_accuracy: 0.9302\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 19/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.1000 - accuracy: 0.9629 - val_loss: 0.1675 - val_accuracy: 0.9382\n",
      "Epoch 20/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0893 - accuracy: 0.9670 - val_loss: 0.1691 - val_accuracy: 0.9378\n",
      "Epoch 21/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0905 - accuracy: 0.9668 - val_loss: 0.1688 - val_accuracy: 0.9388\n",
      "Epoch 22/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0863 - accuracy: 0.9689 - val_loss: 0.1730 - val_accuracy: 0.9374\n",
      "Epoch 23/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0829 - accuracy: 0.9699 - val_loss: 0.1776 - val_accuracy: 0.9364\n",
      "Epoch 24/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0814 - accuracy: 0.9708 - val_loss: 0.1739 - val_accuracy: 0.9368\n",
      "Epoch 25/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0832 - accuracy: 0.9700 - val_loss: 0.1722 - val_accuracy: 0.9377\n",
      "Epoch 26/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0801 - accuracy: 0.9711 - val_loss: 0.1762 - val_accuracy: 0.9379\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 27/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0761 - accuracy: 0.9726 - val_loss: 0.1726 - val_accuracy: 0.9388\n",
      "Epoch 28/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0738 - accuracy: 0.9735 - val_loss: 0.1727 - val_accuracy: 0.9384\n",
      "Epoch 29/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0747 - accuracy: 0.9732 - val_loss: 0.1735 - val_accuracy: 0.9381\n",
      "Epoch 30/30\n",
      "1815/1815 [==============================] - 44s 24ms/step - loss: 0.0757 - accuracy: 0.9727 - val_loss: 0.1741 - val_accuracy: 0.9380\n",
      "Trained model saved\n"
     ]
    }
   ],
   "source": [
    "model_segnet = SegNet()\n",
    "model_segnet.summary()\n",
    "\n",
    "print('Training')\n",
    "batch_size = 8\n",
    "nb_epoch = 30\n",
    "\n",
    "\n",
    "mcp_save = ModelCheckpoint('weight_isic18_segnet', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "history = model_segnet.fit(tr_data,tr_mask,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epoch,\n",
    "              shuffle=True,\n",
    "              verbose=1,\n",
    "              validation_data=(val_data, val_mask), callbacks=[mcp_save, reduce_lr_loss] )\n",
    "\n",
    "model_segnet.save('segnet.h5')\n",
    "  \n",
    "print('Trained model saved')\n",
    "with open('hist_isic18_segnet', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "te_data    = Test_img\n",
    "te_mask = Test_mask\n",
    "te_mask  = np.expand_dims(te_mask, axis=3)\n",
    "te_data2  = dataset_normalized(te_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SegNet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 64, 64, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 128, 128, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 128, 128, 128)     295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 256, 256, 64)      73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 256, 256, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 256, 256, 1)       65        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 256, 256, 1)       0         \n",
      "=================================================================\n",
      "Total params: 10,190,977\n",
      "Trainable params: 10,185,089\n",
      "Non-trainable params: 5,888\n",
      "_________________________________________________________________\n",
      "320/320 [==============================] - 3s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "model_segnet = SegNet()\n",
    "model_segnet.summary()\n",
    "model_segnet.load_weights('weight_isic18_segnet')\n",
    "predictions = model_segnet.predict(te_data, batch_size=8, verbose=1)\n",
    "y_scores = predictions.reshape(predictions.shape[0]*predictions.shape[1]*predictions.shape[2]*predictions.shape[3], 1)\n",
    "y_true = te_mask.reshape(te_mask.shape[0]*te_mask.shape[1]*te_mask.shape[2]*te_mask.shape[3], 1)\n",
    "y_scores = np.where(y_scores>0.5, 1, 0)\n",
    "y_true   = np.where(y_true>0.5, 1, 0)\n",
    "\n",
    "threshold_confusion = 0.5\n",
    "y_pred = np.empty((y_scores.shape[0]))\n",
    "for i in range(y_scores.shape[0]):\n",
    "    if y_scores[i]>=threshold_confusion:\n",
    "        y_pred[i]=1\n",
    "    else:\n",
    "        y_pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 score (F-measure): 0.8454459142483662\n"
     ]
    }
   ],
   "source": [
    "F1_score = f1_score(y_true, y_pred, labels=None, average='binary', sample_weight=None)\n",
    "print (\"\\nF1 score (F-measure): \" +str(F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard similarity score: 0.940300703048706\n"
     ]
    }
   ],
   "source": [
    "jaccard_index = jaccard_similarity_score(y_true, y_pred, normalize=True)\n",
    "print (\"\\nJaccard similarity score: \" +str(jaccard_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
